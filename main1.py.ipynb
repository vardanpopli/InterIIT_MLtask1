{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3793129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49ff3f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The cat is taking a nap.' 'People are enjoying the outdoors.'\n",
      " 'A meal is being cooked.' 'The rain has stopped.'\n",
      " 'The teacher is giving a presentation.' 'The dog is playing fetch.'\n",
      " 'The sky is full of vibrant colors.'\n",
      " 'An experiment is being carried out.' 'Kids are playing in the sand.'\n",
      " 'The hiker successfully reached the top.'\n",
      " 'Someone is creating a painting.' 'A child is laughing on the swing.'\n",
      " 'Cars are driving on the road.' 'People are having a dance party.'\n",
      " 'The room is filled with warmth.'\n",
      " 'Plans for construction are being made.' 'The wind is gently blowing.'\n",
      " 'Someone is celebrating a birthday.' 'The flowers are seeking sunlight.'\n",
      " 'Someone is completing a race.' 'People are eating outdoors.'\n",
      " 'Microscopic examination is taking place.'\n",
      " 'Kids are enjoying the fair rides.' 'Someone is performing music.'\n",
      " 'The library has organized bookshelves.' 'Bread has been baked.'\n",
      " 'Someone is tending to the garden.' 'The child is amused by the toy.'\n",
      " 'The couple is enjoying a romantic stroll.'\n",
      " 'Instruction is being provided in the classroom.'\n",
      " 'Coffee is being prepared.' 'Someone is flying a colorful kite.'\n",
      " 'A wedding celebration is happening.'\n",
      " 'Someone is performing a rescue operation.'\n",
      " 'Someone is scoring points in basketball.'\n",
      " 'Vehicle maintenance is taking place.'\n",
      " 'Chemical substances are being combined.' 'Kids are playing in the snow.'\n",
      " 'Food is being garnished.' 'Someone is working on a literary piece.'\n",
      " 'Pool safety is being monitored.' 'Construction is in progress.'\n",
      " 'Someone is showcasing juggling skills.'\n",
      " 'Baking activities are happening.' 'Ants are being studied.'\n",
      " 'Someone is playing with bubbles.' 'Sheep are being guided by a farmer.'\n",
      " 'Music is being performed on a piano.'\n",
      " 'Dental examination is taking place.' 'Food is being served.'\n",
      " 'Classroom participation is happening.'\n",
      " 'Artwork is being painted on a wall.'\n",
      " 'Bicycle riding instruction is underway.' 'A celebration is in progress.'\n",
      " 'Surgical procedure is underway.' 'Someone is embracing a dog.'\n",
      " 'Nectar is being gathered by a bee.'\n",
      " 'Geological analysis is taking place.' 'A photograph is being taken.'\n",
      " 'Team encouragement is happening.' 'Tailoring work is in progress.'\n",
      " 'Tourism and education are combined.' 'Balloons are being sold.'\n",
      " 'Medication is being provided.' 'Emergency response is happening.'\n",
      " 'Chemical reaction is occurring.' 'Orchestral performance is happening.'\n",
      " 'Educational assistance is given.' 'Vegetables are being prepared.'\n",
      " 'Firefighting operation is underway.' 'Artistic sculpting is occurring.'\n",
      " 'Parent-child bonding is happening.' 'Soccer gameplay is occurring.'\n",
      " 'Patient care is being provided.' 'Apple picking is underway.'\n",
      " 'Construction equipment is in use.' 'Dance performance is happening.'\n",
      " 'Haircutting service is provided.'\n",
      " 'Astronomical observation is happening.'\n",
      " 'Swimming instruction is underway.' 'Socializing around the fire.'\n",
      " 'Mural painting is happening.' 'Pre-game preparation is happening.'\n",
      " 'Flower arrangement is taking place.'\n",
      " 'Scientific experimentation is happening.'\n",
      " 'Jazz music performance is happening.'\n",
      " 'Educational experiment is happening.' 'Pancake cooking is underway.'\n",
      " 'Firefighter training is happening.' 'Artistic sculpting is occurring.'\n",
      " 'Parent-child bonding is happening.' 'Soccer gameplay is occurring.'\n",
      " 'Patient support is being provided.' 'Egg collection is happening.'\n",
      " 'Welding work is happening.' 'Dance performance is happening.'\n",
      " 'Haircutting service is provided.' 'Data analysis is underway.'\n",
      " 'Skateboarding instruction is happening.' 'Socializing around the fire.'\n",
      " 'Astronomical research is underway.' 'Enjoying outdoor activities.'\n",
      " 'Chemical experiment is happening.' 'Culinary creativity is showcased.'\n",
      " 'Celebrating a triumphant win.' 'Pottery creation is taking place.'\n",
      " 'Engaging in educational crafting.'\n",
      " 'Butterfly migration study is underway.' 'Mindful relaxation.'\n",
      " 'Jewelry craftsmanship.' 'Life-saving medical procedure.'\n",
      " 'Chainsaw carving artistry.' 'Expert piloting.'\n",
      " 'Particle physics investigation.' 'Community service.'\n",
      " 'Creative fashion design.' 'Marine mammal observation.'\n",
      " 'Adventurous climbing.' 'Street photography.' 'Educational gardening.'\n",
      " 'Electric car development.' 'Artistic dance expression.'\n",
      " 'Innovative culinary art.' 'Extreme solo climbing.'\n",
      " 'Community art project.' 'Wildlife conservation photography.'\n",
      " 'AI technology development.' 'Environmental advocacy.'\n",
      " 'Soulful jazz performance.' 'Archaeological discovery.'\n",
      " 'Interactive culinary presentation.' 'Renewable energy innovation.'\n",
      " 'Dedicated ballet practice.' 'Whale migration study.'\n",
      " 'Charitable fundraising.' 'Advanced robotics engineering.'\n",
      " 'Eloquent classical performance.' 'Cave geological study.'\n",
      " 'Beach conservation effort.' 'Artistic glassblowing.'\n",
      " 'Cutting-edge medical research.' 'Mural storytelling.'\n",
      " 'High-altitude mountaineering.' 'Nocturnal marine research.'\n",
      " 'Sustainable architecture.' 'Artisanal woodworking.'\n",
      " 'Astronomy appreciation.' 'Cultural fusion fashion.'\n",
      " 'Mindfulness research.' 'Celebration culinary art.'\n",
      " 'Recycled art creation.' 'Smart city innovation.'\n",
      " 'Expressive spoken word art.' 'Sea turtle conservation research.'\n",
      " 'Epic road adventure.' 'Theoretical mathematics breakthrough.'\n",
      " 'Advanced drone technology.' 'Unity-themed mural painting.'\n",
      " 'Deep-sea marine documentation.' 'Storytelling bonding experience.'\n",
      " 'Algae biofuel research.' 'Social commentary through dance.'\n",
      " 'Zero-waste retail concept.' 'Tech-driven social change.'\n",
      " 'Coral reef restoration film.' 'Epidemiological modeling.'\n",
      " 'Advanced prosthetic technology.' 'International dance showcase.'\n",
      " 'Rainforest wildlife photography.' 'Language revitalization research.'\n",
      " 'Community gardening initiative.' 'Sleep optimization technology.'\n",
      " 'Green chemistry innovation.' 'Temporal art installation.'\n",
      " 'Humpback whale research.' 'Universal design playground.'\n",
      " 'Speculative fiction collaboration.' 'Space innovation hackathon.'\n",
      " 'Biodegradable packaging research.' 'Hybrid orchestral composition.'\n",
      " 'Rare bird conservation study.' 'Modular housing innovation.'\n",
      " 'Ecosystem restoration initiative.' 'Assistive exoskeleton technology.'\n",
      " 'Poetry for social change.' 'Quantum algorithm research.'\n",
      " 'Polar climate research.' 'Artisanal jewelry creation.'\n",
      " 'Coral reef ecosystem research.' 'Open education initiative.'\n",
      " 'Community-centered park design.' 'Volunteer mobilization platform.'\n",
      " 'Nanomedicine cancer research.' 'Diversity-themed sculpture project.'\n",
      " 'Consciousness neuroscience research.' \"Women's empowerment initiative.\"\n",
      " 'Community revitalization through art.' 'Dolphin communication study.'\n",
      " 'Emergency water purification technology.'\n",
      " 'Humanitarian architecture design.' 'Mindfulness cognition research.'\n",
      " 'Research collaboration software.' 'Nature-inspired ceramic art.'\n",
      " 'Coral reef conservation advocacy.' 'Misinformation analysis research.'\n",
      " 'Gamified education platform.' 'Upcycled fashion creation.'\n",
      " 'Urban revitalization project.' 'Nature-based mental health research.'\n",
      " 'Stress management wearable technology.'\n",
      " 'Solar panel material development.'\n",
      " \"Children's literature for social values.\" 'Deep-sea ROV exploration.'\n",
      " 'Affordable medical technology.' 'Farm-to-table culinary challenge.'\n",
      " 'Community-driven art installation.'\n",
      " 'Neuroscience of creativity research.' 'Reforestation activism.'\n",
      " 'Regenerative farming movement.' 'AI-powered language education.'\n",
      " 'Kinetic dance expression.' 'Asteroid mining innovation.'\n",
      " 'Urban nature and mental well-being research.'\n",
      " 'Eco-packaging entrepreneurship.' 'Tidal energy innovation.'\n",
      " 'Disaster-resistant housing design.' 'Plastic reduction advocacy.'\n",
      " 'Magnetic levitation transportation.' 'Activist street art movement.'\n",
      " 'Culturally inclusive education.' 'Stem cell cardiac therapy research.'\n",
      " 'Rainforest-inspired symphonic composition.'\n",
      " 'Community solar power initiative.' 'Vertical farming innovation.'\n",
      " 'Ancient civilization excavation.' 'Cultured meat technology.'\n",
      " 'Historic district revitalization.' 'Global reforestation effort.'\n",
      " 'Virtual reality education research.'\n",
      " \"Children's literature for multiculturalism.\"\n",
      " 'Solar-integrated architectural technology.'\n",
      " 'Dark matter particle research.' 'Coastal plastic pollution activism.'\n",
      " 'Urban community gardening.' 'Efficient smart irrigation technology.'\n",
      " 'Community service engagement app.'\n",
      " 'Endangered species habitat conservation.'\n",
      " 'Advanced aerospace materials research.' 'Educational game development.'\n",
      " 'Energy-efficient architectural design.'\n",
      " 'Music therapy and mental health research.'\n",
      " 'Rural renewable energy entrepreneurship.'\n",
      " 'Rainforest plant medicine research.'\n",
      " 'Indigenous land rights documentary.'\n",
      " 'Wave energy conversion innovation.' 'Medical image diagnosis AI.'\n",
      " 'Community-centered urban design.' 'Multimedia storytelling experience.'\n",
      " 'Agroforestry for land restoration.' 'Neuroscience of empathy research.'\n",
      " 'Fair-trade fashion entrepreneurship.' 'Clean water access project.'\n",
      " 'Ancient city excavation project.'\n",
      " 'Genomic personalized medicine research.'\n",
      " 'E-bike urban mobility innovation.' 'Wetland conservation advocacy.'\n",
      " 'Arts-integrated STEAM education.' 'Community wind energy initiative.'\n",
      " 'Fungi-based packaging research.' 'Interactive light art installation.'\n",
      " 'Solar desalination innovation.' 'Smart city technology development.'\n",
      " 'Nature-based education curriculum.' 'Marine protected area advocacy.'\n",
      " 'Algae-based carbon capture research.'\n",
      " 'Regenerative agriculture movement.' 'Zero-waste fashion collection.'\n",
      " 'Meditation and brain plasticity research.'\n",
      " 'Volunteer engagement technology.'\n",
      " 'Battery storage technology innovation.'\n",
      " 'Cultural diversity mural project.' 'Urban food gardening project.'\n",
      " 'Nature sound therapy research.' 'Communal eco-housing design.'\n",
      " 'Wildlife protection drone technology.'\n",
      " 'Resilience storytelling podcast.' 'School solar power initiative.'\n",
      " 'Biodegradable plastic research.' 'Pollinator protection advocacy.'\n",
      " 'Carbon footprint tracking app.' 'Modular eco-friendly furniture design.'\n",
      " 'Nature and well-being research.'\n",
      " 'Permaculture food production movement.'\n",
      " 'Everyday sustainability education.'\n",
      " 'Geothermal energy technology innovation.' 'Recycled sculpture art.'\n",
      " 'Outdoor wellness gatherings.' 'Next-generation solar panel technology.'\n",
      " 'Graphic novel social commentary.'\n",
      " 'Electric bus urban mobility innovation.'\n",
      " 'Coral reef fish behavior research.'\n",
      " 'Eco-conscious education curriculum.' 'Wind power community initiative.'\n",
      " 'Algae-based biofuel research.' 'Art, science, and tech exhibition.'\n",
      " 'Green office space design.' 'Nature-based mental health research.'\n",
      " 'Microvolunteering platform.' 'Solar concentrator innovation.'\n",
      " 'Fungi-based pollution cleanup research.'\n",
      " 'Endangered species art installation.' 'Urban farming for food access.'\n",
      " 'Efficient irrigation technology.' 'Environmental cleanup app.'\n",
      " 'Wildlife corridor preservation.'\n",
      " 'Biodegradable packaging materials research.'\n",
      " 'Cultural diversity mural project.']\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"C:\\\\Users\\\\varda\\\\Documents\\\\mltask1\\\\datasetmltask1.csv\")\n",
    "#print(data1)\n",
    "data = data1.iloc[:,1].values\n",
    "label = data1.iloc[:,-1].values\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f397b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e87d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat', 'taking', 'nap'], ['people', 'enjoying', 'outdoors']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    words = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return words\n",
    "\n",
    "\n",
    "# creating new variable to store tokenized data\n",
    "tokenized_data = []\n",
    "\n",
    "for text in data:\n",
    "    # Preprocess the text\n",
    "    preprocessed_words = preprocess_text(text)\n",
    "    tokenized_data.append(preprocessed_words)\n",
    "\n",
    "# You can access the preprocessed and tokenized words of a specific text, for example, the first text:\n",
    "first_text_tokens = tokenized_data[0:2]\n",
    "print(first_text_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1772194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37776e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "555a6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define a function for data augmentation\n",
    "def augment_data(text, n=1):\n",
    "    augmented_data = [text]\n",
    "    # we can use a thesaurus or a library like NLTK or spaCy to find synonyms\n",
    "    # For simplicity, i am just adding some random words as \"synonyms\"\n",
    "    for _ in range(n):\n",
    "        augmented_text = list(text)  # Copy the original text\n",
    "        for i in range(len(text)):\n",
    "            if random.random() < 0.9:  # Augment 10% of tokens (adjust as needed)\n",
    "                # Replace with a random \"synonym\" (for simplicity, we use random words)\n",
    "                augmented_text[i] = random.choice([\"augmented\", \"text\", \"sample\"])\n",
    "        augmented_data.append(augmented_text)\n",
    "    return augmented_data\n",
    "\n",
    "# Define a function to generate negative samples\n",
    "def generate_negative_samples(tokenized_data, n=1):\n",
    "    negative_samples = []\n",
    "    for text in tokenized_data:\n",
    "        for _ in range(n):\n",
    "            # Randomly select a different text from your dataset as a negative sample\n",
    "            negative_text = random.choice(tokenized_data)\n",
    "            negative_samples.append(negative_text)\n",
    "    return negative_samples\n",
    "\n",
    "# Initialize lists to store augmented and negative samples\n",
    "augmented_tokenized_data = []\n",
    "negative_tokenized_data = []\n",
    "augmented_labels = []\n",
    "negative_labels = []\n",
    "\n",
    "# Iterate through your tokenized data and labels\n",
    "for text, label in zip(tokenized_data, label):\n",
    "    # Add the original text and label\n",
    "    augmented_tokenized_data.append(text)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    # Augment the original text (e.g., create 2 augmented versions)\n",
    "    augmented_texts = augment_data(text, n=10)\n",
    "    augmented_tokenized_data.extend(augmented_texts)\n",
    "    augmented_labels.extend([label] * (len(augmented_texts)))\n",
    "    \n",
    "    # Generate negative samples (e.g., create 3 negative samples)\n",
    "    negative_texts = generate_negative_samples(tokenized_data, n=10)\n",
    "    negative_tokenized_data.extend(negative_texts)\n",
    "    negative_labels.extend([0] * (len(negative_texts)) if label == 1 else [1] * (len(negative_texts)))\n",
    "\n",
    "# Now, 'augmented_tokenized_data' contains augmented versions of the original text, and 'negative_tokenized_data' contains negative samples.\n",
    "# 'augmented_labels' contains labels for augmented data, and 'negative_labels' contains labels for negative samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07ee93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already created 'augmented_tokenized_data' and 'negative_tokenized_data'\n",
    "\n",
    "# Merge the two datasets\n",
    "merged_tokenized_data = augmented_tokenized_data + negative_tokenized_data\n",
    "\n",
    "# Assuming you also have 'augmented_labels' and 'negative_labels' for the respective datasets\n",
    "# Merge the corresponding labels\n",
    "merged_labels = augmented_labels + negative_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c1b600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your preprocessed and tokenized data stored in 'preprocessed_tokenized_data'\n",
    "# Create a vocabulary from your tokenized data\n",
    "vocabulary = set(word for words in merged_tokenized_data for word in words)\n",
    "\n",
    "# Create a dictionary to map words to unique indices\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create one-hot encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Transform each text into a one-hot encoded vector\n",
    "one_hot_encoded_data = []\n",
    "\n",
    "for words in merged_tokenized_data:\n",
    "    one_hot_vector = np.zeros(len(vocabulary), dtype=int)\n",
    "    for word in words:\n",
    "        word_index = word_to_index[word]\n",
    "        one_hot_vector[word_index] = 1\n",
    "    one_hot_encoded_data.append(one_hot_vector)\n",
    "\n",
    "# one_hot_encoded_data now contains a list of one-hot encoded vectors, one for each text\n",
    "\n",
    "# You can access the one-hot encoded vector of a specific text, for example, the first text:\n",
    "first_text_one_hot = one_hot_encoded_data[0]\n",
    "\n",
    "# You can also print the one-hot encoded vector of the first text\n",
    "print(first_text_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473203be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming you have 'one_hot_encoded_data' as your one-hot encoded dataset\n",
    "# and 'labels' as your corresponding labels (e.g., 'Satisfies' or 'Does not satisfy')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = one_hot_encoded_data\n",
    "y = merged_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a simple multi-layer perceptron (MLP) classifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de724d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.64%\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f24210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc9869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d751b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43df05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e4774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3a3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
