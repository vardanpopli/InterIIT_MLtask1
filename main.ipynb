{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59db833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy  # Import spaCy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0e31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasetmltask1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a85095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102fe2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "premise       317\n",
       "hypothesis    309\n",
       "label           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd04549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               premise  \\\n",
      "70   A sculptor is shaping a block of marble into a...   \n",
      "71   A parent is helping their child fly a kite in ...   \n",
      "72   A soccer player is dribbling the ball down the...   \n",
      "76   A dancer is performing a graceful ballet routine.   \n",
      "77   A barber is giving a customer a haircut at the...   \n",
      "80   A group of friends is laughing and chatting ar...   \n",
      "89   A sculptor is chiseling a piece of marble into...   \n",
      "90   A parent is helping their child build a sandca...   \n",
      "91   A soccer player is taking a penalty kick durin...   \n",
      "95    A dancer is performing a lively hip-hop routine.   \n",
      "96   A barber is giving a customer a stylish haircu...   \n",
      "99   A group of friends is sharing stories around a...   \n",
      "208  A group of psychologists is conducting studies...   \n",
      "280  A team of artists is creating a public mural t...   \n",
      "306  A team of psychologists is researching the eff...   \n",
      "316  A group of artists is creating a series of pub...   \n",
      "\n",
      "                               hypothesis  label  \n",
      "70       Artistic sculpting is occurring.      1  \n",
      "71     Parent-child bonding is happening.      1  \n",
      "72          Soccer gameplay is occurring.      1  \n",
      "76        Dance performance is happening.      1  \n",
      "77       Haircutting service is provided.      1  \n",
      "80           Socializing around the fire.      1  \n",
      "89       Artistic sculpting is occurring.      1  \n",
      "90     Parent-child bonding is happening.      1  \n",
      "91          Soccer gameplay is occurring.      1  \n",
      "95        Dance performance is happening.      1  \n",
      "96       Haircutting service is provided.      1  \n",
      "99           Socializing around the fire.      1  \n",
      "208  Nature-based mental health research.      1  \n",
      "280     Cultural diversity mural project.      1  \n",
      "306  Nature-based mental health research.      1  \n",
      "316     Cultural diversity mural project.      1  \n"
     ]
    }
   ],
   "source": [
    "duplicate_hypotheses = df[df.duplicated(subset='hypothesis', keep=False)]\n",
    "\n",
    "# Print the rows with duplicate hypotheses\n",
    "print(duplicate_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1111768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function\n",
    "def generate_negative_samples(df, model, sentence_vecs1, sentence_vecs2):\n",
    "    negative_samples = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        negative_samples.append((df['premise'][i], df['hypothesis'][i], 1))\n",
    "\n",
    "    for premise_idx, premise in enumerate(df['premise']):\n",
    "        premise_vec = sentence_vecs1[premise_idx].reshape(1, -1)\n",
    "        hypothesis_vecs = sentence_vecs2\n",
    "\n",
    "        # Compute cosine similarities between the premise and all hypotheses\n",
    "        similarities = cosine_similarity(premise_vec, hypothesis_vecs)[0]\n",
    "\n",
    "        # Find the indices of 5 hypotheses with the least similarity\n",
    "        least_similar_indices = np.argpartition(similarities, 5)[:5]\n",
    "\n",
    "        # Append these hypotheses to negative_samples with label 0\n",
    "        for idx in least_similar_indices:\n",
    "            negative_samples.append((premise, df['hypothesis'][idx], 0))\n",
    "\n",
    "    return negative_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70538137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples saved to augmented_dataset_ml_task1.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences1 = tokenizer(df['premise'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_sentences2 = tokenizer(df['hypothesis'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Encode the tokenized sequences\n",
    "encoded_sentences1 = model(**tokenized_sentences1).last_hidden_state\n",
    "encoded_sentences2 = model(**tokenized_sentences2).last_hidden_state\n",
    "\n",
    "# Average pooling to get sentence-level embeddings\n",
    "sentence_vecs1 = np.mean(encoded_sentences1.detach().numpy(), axis=1)\n",
    "sentence_vecs2 = np.mean(encoded_sentences2.detach().numpy(), axis=1)\n",
    "\n",
    "# Generate negative samples\n",
    "negative_samples = generate_negative_samples(df, model, sentence_vecs1, sentence_vecs2)\n",
    "\n",
    "# Create a DataFrame from the negative_samples list\n",
    "df_negative_samples = pd.DataFrame(negative_samples, columns=['premise', 'hypothesis', 'label'])\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = 'augmented_dataset_ml_task1.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_negative_samples.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Negative samples saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cb7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset from 'augmented_dataset_ml_task1.csv' (replace with your actual file path)\n",
    "df = pd.read_csv('augmented_dataset_ml_task1.csv')\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify common rows between train and test datasets\n",
    "common_rows = pd.merge(train_df, test_df, on=['premise', 'hypothesis'], how='inner')\n",
    "\n",
    "# Remove common rows from the test dataset\n",
    "test_df = test_df[~test_df.index.isin(common_rows.index)]\n",
    "\n",
    "# Save the train and test sets to CSV files\n",
    "train_df.to_csv('train_dataset.csv', index=False)  # Save train dataset to 'train_dataset.csv'\n",
    "test_df.to_csv('test_dataset.csv', index=False)    # Save updated test dataset to 'test_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e1e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        premise = str(self.data.loc[index, 'premise'])\n",
    "        hypothesis = str(self.data.loc[index, 'hypothesis'])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(self.data.loc[index, 'label'])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796d9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model with a custom classifier\n",
    "def define_model(hidden_size, num_labels):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    \n",
    "    classifier = nn.Sequential(\n",
    "        nn.Linear(model.config.hidden_size, hidden_size),\n",
    "        nn.BatchNorm1d(hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(hidden_size, num_labels)\n",
    "    )\n",
    "    \n",
    "    model.classifier = classifier\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train and validate the model for one epoch\n",
    "def train_epoch(model, train_dataloader, val_dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_predictions_train = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        correct_predictions_train += torch.sum(predicted_labels == labels).item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions_train / total_train_samples\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions_val = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            correct_predictions_val += torch.sum(predicted_labels == labels).item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "            val_loss = loss_fn(logits, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions_val / total_val_samples\n",
    "\n",
    "    return average_train_loss, train_accuracy, average_val_loss, val_accuracy\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "def hyperparameter_tuning(learning_rates, batch_sizes, hidden_sizes, num_epochs, max_length, weight_decay, df):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_learning_rate = 0.0\n",
    "    best_batch_size = 0\n",
    "    best_hidden_size = 0\n",
    "    best_model = None\n",
    "\n",
    "    train_losses_all = []\n",
    "    val_losses_all = []\n",
    "    train_accuracies_all = []\n",
    "    val_accuracies_all = []\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                print(f\"Training with learning rate: {learning_rate}, batch size: {batch_size}, hidden size: {hidden_size}\")\n",
    "\n",
    "                # Set up the model\n",
    "                model = define_model(hidden_size, num_labels=2).to(device)\n",
    "                optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                # Create the dataset and dataloaders\n",
    "                dataset = CustomDataset(df, tokenizer, max_length)\n",
    "                train_size = int(0.7 * len(dataset))\n",
    "                val_size = len(dataset) - train_size\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "                train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                # Lists to store training and validation metrics for each epoch\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                train_accuracies = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    train_loss, train_accuracy, val_loss, val_accuracy = train_epoch(\n",
    "                        model, train_dataloader, val_dataloader, optimizer, loss_fn, device\n",
    "                    )\n",
    "\n",
    "                    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "                    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "                    print(\"*****************************************************\")\n",
    "\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    train_accuracies.append(train_accuracy)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "                # Check if the current validation accuracy is the best so far\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    best_learning_rate = learning_rate\n",
    "                    best_batch_size = batch_size\n",
    "                    best_hidden_size = hidden_size\n",
    "                    best_model = model.state_dict()\n",
    "\n",
    "                   # Save the model with the best validation accuracy\n",
    "                    best_model_path = f\"best_model_lr_{best_learning_rate}_batch_{best_batch_size}_hidden_{best_hidden_size}.pt\"\n",
    "                    torch.save(model, best_model_path)\n",
    "\n",
    "                # Store the metrics for this combination\n",
    "                train_losses_all.append(train_losses)\n",
    "                val_losses_all.append(val_losses)\n",
    "                train_accuracies_all.append(train_accuracies)\n",
    "                val_accuracies_all.append(val_accuracies)\n",
    "\n",
    "                idx += 1\n",
    "                # Set the model back to training mode\n",
    "                model.train()\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    # Print the best hyperparameters and accuracy\n",
    "    print(f\"Best Learning Rate: {best_learning_rate}\")\n",
    "    print(f\"Best Batch Size: {best_batch_size}\")\n",
    "    print(f\"Best Hidden Size: {best_hidden_size}\")\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # Return relevant information\n",
    "    return best_model, best_learning_rate, best_batch_size, best_hidden_size, train_losses_all, val_losses_all, train_accuracies_all, val_accuracies_all\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae738d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.0001, batch size: 8, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6328 | Train Accuracy: 0.6353\n",
      "Validation Loss: 0.7148 | Validation Accuracy: 0.1707\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5570 | Train Accuracy: 0.7481\n",
      "Validation Loss: 0.2924 | Validation Accuracy: 0.8753\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3328 | Train Accuracy: 0.9041\n",
      "Validation Loss: 0.2212 | Validation Accuracy: 0.9453\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2615 | Train Accuracy: 0.9370\n",
      "Validation Loss: 0.2198 | Validation Accuracy: 0.9562\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2015 | Train Accuracy: 0.9615\n",
      "Validation Loss: 0.1396 | Validation Accuracy: 0.9803\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 8, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5494 | Train Accuracy: 0.7105\n",
      "Validation Loss: 0.2938 | Validation Accuracy: 0.9300\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4184 | Train Accuracy: 0.8496\n",
      "Validation Loss: 0.4398 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3149 | Train Accuracy: 0.8966\n",
      "Validation Loss: 0.3303 | Validation Accuracy: 0.9059\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1996 | Train Accuracy: 0.9539\n",
      "Validation Loss: 0.2353 | Validation Accuracy: 0.9475\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1306 | Train Accuracy: 0.9803\n",
      "Validation Loss: 0.1416 | Validation Accuracy: 0.9869\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 8, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5940 | Train Accuracy: 0.6889\n",
      "Validation Loss: 0.4949 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4877 | Train Accuracy: 0.8233\n",
      "Validation Loss: 0.5140 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4847 | Train Accuracy: 0.8299\n",
      "Validation Loss: 0.6212 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4802 | Train Accuracy: 0.8252\n",
      "Validation Loss: 0.5245 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4935 | Train Accuracy: 0.8327\n",
      "Validation Loss: 0.5728 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0001, batch size: 16, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.4675 | Train Accuracy: 0.8083\n",
      "Validation Loss: 0.2892 | Validation Accuracy: 0.9540\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.2824 | Train Accuracy: 0.9605\n",
      "Validation Loss: 0.1932 | Validation Accuracy: 0.9912\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1810 | Train Accuracy: 0.9887\n",
      "Validation Loss: 0.0987 | Validation Accuracy: 0.9956\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1664 | Train Accuracy: 0.9821\n",
      "Validation Loss: 0.0982 | Validation Accuracy: 0.9847\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1599 | Train Accuracy: 0.9784\n",
      "Validation Loss: 0.0830 | Validation Accuracy: 0.9891\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 16, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.4604 | Train Accuracy: 0.7820\n",
      "Validation Loss: 0.0895 | Validation Accuracy: 0.9847\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.1641 | Train Accuracy: 0.9737\n",
      "Validation Loss: 0.2462 | Validation Accuracy: 0.9694\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1586 | Train Accuracy: 0.9671\n",
      "Validation Loss: 0.0786 | Validation Accuracy: 0.9803\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2693 | Train Accuracy: 0.9173\n",
      "Validation Loss: 0.1701 | Validation Accuracy: 0.9781\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0706 | Train Accuracy: 0.9953\n",
      "Validation Loss: 0.1138 | Validation Accuracy: 0.9869\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 16, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5758 | Train Accuracy: 0.7049\n",
      "Validation Loss: 0.4647 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4979 | Train Accuracy: 0.8045\n",
      "Validation Loss: 0.6317 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4866 | Train Accuracy: 0.8224\n",
      "Validation Loss: 0.6247 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4932 | Train Accuracy: 0.8299\n",
      "Validation Loss: 0.5740 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4819 | Train Accuracy: 0.8261\n",
      "Validation Loss: 0.6347 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0001, batch size: 32, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5399 | Train Accuracy: 0.7547\n",
      "Validation Loss: 0.5645 | Validation Accuracy: 0.8337\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3737 | Train Accuracy: 0.8835\n",
      "Validation Loss: 0.1420 | Validation Accuracy: 0.9540\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1922 | Train Accuracy: 0.9765\n",
      "Validation Loss: 0.1546 | Validation Accuracy: 0.9781\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1562 | Train Accuracy: 0.9878\n",
      "Validation Loss: 0.1383 | Validation Accuracy: 0.9891\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1250 | Train Accuracy: 0.9944\n",
      "Validation Loss: 0.0761 | Validation Accuracy: 0.9912\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 32, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6873 | Train Accuracy: 0.5949\n",
      "Validation Loss: 0.5336 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5735 | Train Accuracy: 0.7321\n",
      "Validation Loss: 0.4004 | Validation Accuracy: 0.8403\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3459 | Train Accuracy: 0.8919\n",
      "Validation Loss: 0.2266 | Validation Accuracy: 0.9606\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1314 | Train Accuracy: 0.9887\n",
      "Validation Loss: 0.2358 | Validation Accuracy: 0.9628\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1404 | Train Accuracy: 0.9840\n",
      "Validation Loss: 0.0735 | Validation Accuracy: 0.9934\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0001, batch size: 32, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6207 | Train Accuracy: 0.6297\n",
      "Validation Loss: 0.3421 | Validation Accuracy: 0.9190\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.1864 | Train Accuracy: 0.9737\n",
      "Validation Loss: 0.0789 | Validation Accuracy: 0.9891\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1500 | Train Accuracy: 0.9756\n",
      "Validation Loss: 0.0728 | Validation Accuracy: 0.9912\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1225 | Train Accuracy: 0.9812\n",
      "Validation Loss: 0.0701 | Validation Accuracy: 0.9869\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0591 | Train Accuracy: 0.9972\n",
      "Validation Loss: 0.0561 | Validation Accuracy: 0.9912\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0002, batch size: 8, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5966 | Train Accuracy: 0.7021\n",
      "Validation Loss: 0.5484 | Validation Accuracy: 0.8249\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5063 | Train Accuracy: 0.8139\n",
      "Validation Loss: 0.5974 | Validation Accuracy: 0.8249\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5036 | Train Accuracy: 0.8280\n",
      "Validation Loss: 0.5805 | Validation Accuracy: 0.8249\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4681 | Train Accuracy: 0.8318\n",
      "Validation Loss: 0.5043 | Validation Accuracy: 0.8249\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4712 | Train Accuracy: 0.8365\n",
      "Validation Loss: 0.5339 | Validation Accuracy: 0.8249\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 8, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5954 | Train Accuracy: 0.6842\n",
      "Validation Loss: 0.5147 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4775 | Train Accuracy: 0.8271\n",
      "Validation Loss: 0.6046 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4706 | Train Accuracy: 0.8365\n",
      "Validation Loss: 0.5591 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4688 | Train Accuracy: 0.8402\n",
      "Validation Loss: 0.5618 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4702 | Train Accuracy: 0.8412\n",
      "Validation Loss: 0.5345 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 8, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5281 | Train Accuracy: 0.7820\n",
      "Validation Loss: 0.5462 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4882 | Train Accuracy: 0.8336\n",
      "Validation Loss: 0.4517 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5053 | Train Accuracy: 0.8261\n",
      "Validation Loss: 0.5274 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4866 | Train Accuracy: 0.8289\n",
      "Validation Loss: 0.5321 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4924 | Train Accuracy: 0.8299\n",
      "Validation Loss: 0.5633 | Validation Accuracy: 0.8315\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0002, batch size: 16, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.7191 | Train Accuracy: 0.5395\n",
      "Validation Loss: 0.6257 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5884 | Train Accuracy: 0.7039\n",
      "Validation Loss: 0.6180 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5395 | Train Accuracy: 0.7829\n",
      "Validation Loss: 0.6188 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5171 | Train Accuracy: 0.8130\n",
      "Validation Loss: 0.5339 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4937 | Train Accuracy: 0.8271\n",
      "Validation Loss: 0.5788 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 16, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6246 | Train Accuracy: 0.6570\n",
      "Validation Loss: 0.5447 | Validation Accuracy: 0.7484\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4220 | Train Accuracy: 0.8496\n",
      "Validation Loss: 0.7973 | Validation Accuracy: 0.4967\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4805 | Train Accuracy: 0.8167\n",
      "Validation Loss: 0.5115 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4933 | Train Accuracy: 0.8214\n",
      "Validation Loss: 0.4492 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4827 | Train Accuracy: 0.8289\n",
      "Validation Loss: 0.5004 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 16, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5142 | Train Accuracy: 0.7434\n",
      "Validation Loss: 0.2290 | Validation Accuracy: 0.9475\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.2330 | Train Accuracy: 0.9267\n",
      "Validation Loss: 0.0338 | Validation Accuracy: 0.9891\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2217 | Train Accuracy: 0.9211\n",
      "Validation Loss: 0.1822 | Validation Accuracy: 0.9125\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1823 | Train Accuracy: 0.9483\n",
      "Validation Loss: 0.4520 | Validation Accuracy: 0.8403\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5177 | Train Accuracy: 0.8318\n",
      "Validation Loss: 0.4436 | Validation Accuracy: 0.8403\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0002, batch size: 32, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.7189 | Train Accuracy: 0.5498\n",
      "Validation Loss: 3.1446 | Validation Accuracy: 0.1707\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4606 | Train Accuracy: 0.8308\n",
      "Validation Loss: 0.2298 | Validation Accuracy: 0.9431\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2941 | Train Accuracy: 0.9549\n",
      "Validation Loss: 0.3779 | Validation Accuracy: 0.9781\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.2294 | Train Accuracy: 0.9859\n",
      "Validation Loss: 0.2117 | Validation Accuracy: 0.9759\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2005 | Train Accuracy: 0.9784\n",
      "Validation Loss: 0.2605 | Validation Accuracy: 0.9606\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 32, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.4458 | Train Accuracy: 0.8214\n",
      "Validation Loss: 0.1901 | Validation Accuracy: 0.9584\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.2272 | Train Accuracy: 0.9511\n",
      "Validation Loss: 0.2128 | Validation Accuracy: 0.9497\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2161 | Train Accuracy: 0.9455\n",
      "Validation Loss: 0.2291 | Validation Accuracy: 0.9344\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1889 | Train Accuracy: 0.9521\n",
      "Validation Loss: 0.1172 | Validation Accuracy: 0.9716\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1411 | Train Accuracy: 0.9690\n",
      "Validation Loss: 0.0514 | Validation Accuracy: 0.9847\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0002, batch size: 32, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.4852 | Train Accuracy: 0.7566\n",
      "Validation Loss: 0.5710 | Validation Accuracy: 0.8228\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.1697 | Train Accuracy: 0.9652\n",
      "Validation Loss: 0.0428 | Validation Accuracy: 0.9934\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.3300 | Train Accuracy: 0.8872\n",
      "Validation Loss: 0.7724 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4773 | Train Accuracy: 0.8374\n",
      "Validation Loss: 0.6564 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4697 | Train Accuracy: 0.8318\n",
      "Validation Loss: 0.6653 | Validation Accuracy: 0.8118\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0003, batch size: 8, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5841 | Train Accuracy: 0.7171\n",
      "Validation Loss: 0.6485 | Validation Accuracy: 0.7987\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4883 | Train Accuracy: 0.8280\n",
      "Validation Loss: 0.6074 | Validation Accuracy: 0.7987\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4731 | Train Accuracy: 0.8412\n",
      "Validation Loss: 0.4981 | Validation Accuracy: 0.7987\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4578 | Train Accuracy: 0.8440\n",
      "Validation Loss: 0.5904 | Validation Accuracy: 0.7987\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4670 | Train Accuracy: 0.8430\n",
      "Validation Loss: 0.5573 | Validation Accuracy: 0.7987\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 8, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5696 | Train Accuracy: 0.7368\n",
      "Validation Loss: 0.5980 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4935 | Train Accuracy: 0.8233\n",
      "Validation Loss: 0.5791 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4951 | Train Accuracy: 0.8261\n",
      "Validation Loss: 0.6025 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4977 | Train Accuracy: 0.8261\n",
      "Validation Loss: 0.5531 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4836 | Train Accuracy: 0.8308\n",
      "Validation Loss: 0.5049 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 8, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1302540d-33e6-49da-ad4a-7c9c864888b0)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5618 | Train Accuracy: 0.7538\n",
      "Validation Loss: 0.4715 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4790 | Train Accuracy: 0.8289\n",
      "Validation Loss: 0.5734 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4834 | Train Accuracy: 0.8299\n",
      "Validation Loss: 0.4934 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4821 | Train Accuracy: 0.8299\n",
      "Validation Loss: 0.5574 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4731 | Train Accuracy: 0.8327\n",
      "Validation Loss: 0.4536 | Validation Accuracy: 0.8293\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0003, batch size: 16, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 92c5ca7d-a362-4303-929c-6e97c8723596)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6614 | Train Accuracy: 0.6165\n",
      "Validation Loss: 0.4521 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5606 | Train Accuracy: 0.7707\n",
      "Validation Loss: 0.5604 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4975 | Train Accuracy: 0.8111\n",
      "Validation Loss: 0.4529 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5191 | Train Accuracy: 0.7998\n",
      "Validation Loss: 0.6819 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5020 | Train Accuracy: 0.8130\n",
      "Validation Loss: 0.5449 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 16, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5760 | Train Accuracy: 0.7274\n",
      "Validation Loss: 0.4746 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4887 | Train Accuracy: 0.8177\n",
      "Validation Loss: 0.4510 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4798 | Train Accuracy: 0.8271\n",
      "Validation Loss: 0.5738 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4836 | Train Accuracy: 0.8289\n",
      "Validation Loss: 0.5623 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4956 | Train Accuracy: 0.8261\n",
      "Validation Loss: 0.5144 | Validation Accuracy: 0.8359\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 16, hidden size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.5166 | Train Accuracy: 0.7697\n",
      "Validation Loss: 1.0648 | Validation Accuracy: 0.8162\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.4655 | Train Accuracy: 0.8374\n",
      "Validation Loss: 0.5349 | Validation Accuracy: 0.8162\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4699 | Train Accuracy: 0.8383\n",
      "Validation Loss: 0.5832 | Validation Accuracy: 0.8162\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4723 | Train Accuracy: 0.8393\n",
      "Validation Loss: 0.4731 | Validation Accuracy: 0.8162\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.4713 | Train Accuracy: 0.8355\n",
      "Validation Loss: 0.5403 | Validation Accuracy: 0.8162\n",
      "*****************************************************\n",
      "--------------------------------------------------------------------\n",
      "Training with learning rate: 0.0003, batch size: 32, hidden size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6298 | Train Accuracy: 0.6466\n",
      "Validation Loss: 0.6602 | Validation Accuracy: 0.8381\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5497 | Train Accuracy: 0.7641\n",
      "Validation Loss: 0.4910 | Validation Accuracy: 0.8381\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5205 | Train Accuracy: 0.8148\n",
      "Validation Loss: 0.4478 | Validation Accuracy: 0.8381\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5140 | Train Accuracy: 0.8205\n",
      "Validation Loss: 0.4763 | Validation Accuracy: 0.8381\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5202 | Train Accuracy: 0.8233\n",
      "Validation Loss: 0.4477 | Validation Accuracy: 0.8381\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 32, hidden size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6067 | Train Accuracy: 0.6795\n",
      "Validation Loss: 0.6225 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5288 | Train Accuracy: 0.7942\n",
      "Validation Loss: 0.5285 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 3/5\n",
      "Train Loss: 0.5080 | Train Accuracy: 0.8148\n",
      "Validation Loss: 0.5395 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 4/5\n",
      "Train Loss: 0.4982 | Train Accuracy: 0.8186\n",
      "Validation Loss: 0.5016 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5025 | Train Accuracy: 0.8186\n",
      "Validation Loss: 0.5654 | Validation Accuracy: 0.8534\n",
      "*****************************************************\n",
      "Training with learning rate: 0.0003, batch size: 32, hidden size: 256\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\"), '(Request ID: ec4f54d8-e891-4a8a-b785-058762dd5bd9)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    716\u001b[0m     conn,\n\u001b[0;32m    717\u001b[0m     method,\n\u001b[0;32m    718\u001b[0m     url,\n\u001b[0;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[38;5;241m=\u001b[39mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    800\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    801\u001b[0m )\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Call the hyperparameter tuning function\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m best_model, best_lr, best_batch_size, best_hidden_size, train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m hyperparameter_tuning(\n\u001b[0;32m     11\u001b[0m     learning_rates, batch_sizes, hidden_sizes, num_epochs, max_length, weight_decay, df\n\u001b[0;32m     12\u001b[0m )\n",
      "Cell \u001b[1;32mIn[12], line 101\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m(learning_rates, batch_sizes, hidden_sizes, num_epochs, max_length, weight_decay, df)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Set up the model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m model \u001b[38;5;241m=\u001b[39m define_model(hidden_size, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    102\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    103\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mdefine_model\u001b[1;34m(hidden_size, num_labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_model\u001b[39m(hidden_size, num_labels):\n\u001b[1;32m----> 9\u001b[0m     model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\n\u001b[0;32m     11\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     12\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, hidden_size),\n\u001b[0;32m     13\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(hidden_size),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size, num_labels)\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m classifier\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:2449\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2448\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[1;32m-> 2449\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   2450\u001b[0m         config_path,\n\u001b[0;32m   2451\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2452\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2453\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   2454\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   2455\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   2456\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2457\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2458\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2459\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   2460\u001b[0m         _from_auto\u001b[38;5;241m=\u001b[39mfrom_auto_class,\n\u001b[0;32m   2461\u001b[0m         _from_pipeline\u001b[38;5;241m=\u001b[39mfrom_pipeline,\n\u001b[0;32m   2462\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2463\u001b[0m     )\n\u001b[0;32m   2464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2465\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:591\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[1;32m--> 591\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    593\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    622\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    676\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    677\u001b[0m         configuration_file,\n\u001b[0;32m    678\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    679\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    680\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    681\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    682\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    683\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    684\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    685\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    686\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    687\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    688\u001b[0m     )\n\u001b[0;32m    689\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    429\u001b[0m         path_or_repo_id,\n\u001b[0;32m    430\u001b[0m         filename,\n\u001b[0;32m    431\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    432\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    433\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    434\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    435\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    436\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    437\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    438\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    439\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    440\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1233\u001b[0m             url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1234\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1235\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1236\u001b[0m             timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1237\u001b[0m         )\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1599\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1596\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1599\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1600\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1601\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1602\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1603\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1604\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1605\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1606\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1607\u001b[0m )\n\u001b[0;32m   1608\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:417\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# 2. Force relative redirection\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 417\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    418\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    419\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    420\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m    421\u001b[0m         base_wait_time\u001b[38;5;241m=\u001b[39mbase_wait_time,\n\u001b[0;32m    422\u001b[0m         max_wait_time\u001b[38;5;241m=\u001b[39mmax_wait_time,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    424\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# 3. Exponential backoff\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m http_backoff(\n\u001b[0;32m    453\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    454\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    455\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m    456\u001b[0m     base_wait_time\u001b[38;5;241m=\u001b[39mbase_wait_time,\n\u001b[0;32m    457\u001b[0m     max_wait_time\u001b[38;5;241m=\u001b[39mmax_wait_time,\n\u001b[0;32m    458\u001b[0m     retry_on_exceptions\u001b[38;5;241m=\u001b[39m(Timeout, ProxyError),\n\u001b[0;32m    459\u001b[0m     retry_on_status_codes\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m    460\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    462\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     65\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mSSLError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\"), '(Request ID: ec4f54d8-e891-4a8a-b785-058762dd5bd9)')"
     ]
    }
   ],
   "source": [
    "# Example hyperparameter values\n",
    "learning_rates = [1e-4, 2e-4, 3e-4]\n",
    "batch_sizes = [8, 16, 32]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "num_epochs = 5\n",
    "max_length = 128\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Call the hyperparameter tuning function\n",
    "best_model, best_lr, best_batch_size, best_hidden_size, train_losses, val_losses, train_accuracies, val_accuracies = hyperparameter_tuning(\n",
    "    learning_rates, batch_sizes, hidden_sizes, num_epochs, max_length, weight_decay, df\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b51e42e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Determine the index of the best hyperparameter combination\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(val_accuracies)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plotting the best combination\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# Determine the index of the best hyperparameter combination\n",
    "best_idx = np.argmax(val_accuracies)\n",
    "\n",
    "# Plotting the best combination\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies[best_idx], label='Train Accuracy', marker='o')\n",
    "plt.plot(val_accuracies[best_idx], label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[best_idx], label='Train Loss', marker='o')\n",
    "plt.plot(val_losses[best_idx], label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save or display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1682ec12-94f6-4b77-8938-6dd06bbf6704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "          Predicted 0   Predicted 1\n",
      "Actual 0     317            0\n",
      "Actual 1     2            61\n",
      "Classification report: \n",
      "Precision:  1.0\n",
      "Recall:  0.9682539682539683\n",
      "Test accuracy:  0.9947368421052631\n",
      "f1 score:  0.9973614775725593\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the path to the saved model\n",
    "best_model_path = 'best_model_lr_0.0001_batch_32_hidden_128.pt'\n",
    "\n",
    "# Load the model\n",
    "best_model = torch.load(best_model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_model.to(device)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Set the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the test data\n",
    "test_inputs = tokenizer.batch_encode_plus(\n",
    "    test_df[['premise', 'hypothesis']].values.tolist(),\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "test_input_ids = test_inputs['input_ids'].to(device)\n",
    "test_attention_mask = test_inputs['attention_mask'].to(device)\n",
    "test_labels = torch.tensor(test_df['label'].tolist()).to(device)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = best_model(test_input_ids, attention_mask=test_attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    predicted_labels = predicted_labels.cpu().numpy()\n",
    "    test_labels = test_labels.cpu().numpy()\n",
    "\n",
    "# Calculate TP, TN, FP, FN\n",
    "TP = np.sum((test_labels == 1) & (predicted_labels == 1))\n",
    "TN = np.sum((test_labels == 0) & (predicted_labels == 0))\n",
    "FP = np.sum((test_labels == 0) & (predicted_labels == 1))\n",
    "FN = np.sum((test_labels == 1) & (predicted_labels == 0))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"          Predicted 0   Predicted 1\")\n",
    "print(f\"Actual 0     {TN}            {FP}\")\n",
    "print(f\"Actual 1     {FN}            {TP}\")\n",
    "\n",
    "TP=TP.item()\n",
    "TN=TN.item()\n",
    "FP=FP.item()\n",
    "FN=FN.item()\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "f1 = (2*precision*accuracy)/(precision+accuracy)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('Test accuracy: ', accuracy)\n",
    "print('f1 score: ', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9638d5b8-1245-49e8-9a8e-5bf6d0e2aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the premise:  Vardan is doing hard work this semester\n",
      "Enter the hypothesis:  This semester Vardan is working hard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Satisfying\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "best_model.to(device)\n",
    "\n",
    "# Input premise and hypothesis (modify as needed)\n",
    "premise = input(\"Enter the premise: \")\n",
    "hypothesis = input(\"Enter the hypothesis: \")\n",
    "\n",
    "# Tokenize and encode the input\n",
    "inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "    logits = best_model(**inputs).logits\n",
    "\n",
    "# Determine the predicted class\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Map the predicted class to human-readable labels\n",
    "class_labels = [\"Not Satisfying\", \"Satisfying\"]\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "# Print the prediction\n",
    "print(f\"Prediction: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22c04f-fb2a-4b86-bed6-c0c54aaa6e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
